{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "from gymnasium.spaces import Discrete,MultiDiscrete\n",
    "from pettingzoo import ParallelEnv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROWS = 6\n",
    "COLUMNS = 7\n",
    "EMPTY = 0\n",
    "PLAYER_O = 1\n",
    "PLAYER_X = 2\n",
    "\n",
    "def build_board():\n",
    "    return [[EMPTY for _ in range(COLUMNS)] for _ in range(ROWS)]\n",
    "\n",
    "def drop_token(board, col, token):#drops token\n",
    "    for row in range(ROWS - 1, -1, -1):\n",
    "        if board[row][col] == EMPTY:\n",
    "            board[row][col] = token\n",
    "            return row\n",
    "    raise ValueError(\"Column is full\")\n",
    "\n",
    "def count_aligned(board, row, col, token, d_row, d_col):\n",
    "    total = 0\n",
    "    r, c = row + d_row, col + d_col\n",
    "    while 0 <= r < ROWS and 0 <= c < COLUMNS and board[r][c] == token:\n",
    "        total += 1\n",
    "        r += d_row\n",
    "        c += d_col\n",
    "    return total\n",
    "\n",
    "def has_winner(board, row, col, token):\n",
    "    directions = ((1, 0), (0, 1), (1, 1), (1, -1))\n",
    "    for d_row, d_col in directions:\n",
    "        span = 1 + count_aligned(board, row, col, token, d_row, d_col)\n",
    "        span += count_aligned(board, row, col, token, -d_row, -d_col)\n",
    "        if span >= 4:\n",
    "            return True\n",
    "    return False\n",
    "def column_is_full(board, col):\n",
    "    return board[0][col] != EMPTY\n",
    "\n",
    "def board_is_full(board):\n",
    "    return all(column_is_full(board, col) for col in range(COLUMNS))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CustomEnvironment(ParallelEnv):\n",
    "    metadata = {\n",
    "        \"name\": \"custom_environment_v0\",\n",
    "    }\n",
    "\n",
    "    def __init__(self, maxsteps):\n",
    "        self.maxsteps = maxsteps\n",
    "        self.possible_agents = [\"player_o\", \"player_x\"]\n",
    "        self.action_spaces = {\n",
    "            agent: Discrete(COLUMNS) for agent in self.possible_agents\n",
    "        }\n",
    "        nvec = np.full(ROWS * COLUMNS, 3, dtype=np.int64)\n",
    "        self.observation_spaces = {\n",
    "            agent: MultiDiscrete(nvec, dtype=np.int64)\n",
    "            for agent in self.possible_agents\n",
    "        }\n",
    "        self._reset_internal_state()\n",
    "\n",
    "    def _reset_internal_state(self):\n",
    "        self.grid = build_board()\n",
    "        self.timestep = 0\n",
    "        self.agents = copy(self.possible_agents)\n",
    "        self.last_moves = {agent: None for agent in self.possible_agents}\n",
    "\n",
    "    @staticmethod\n",
    "    def _opponent(agent):\n",
    "        return \"player_x\" if agent == \"player_o\" else \"player_o\"\n",
    "\n",
    "    def _board_to_obs(self):\n",
    "        flat = [cell for row in self.grid for cell in row]\n",
    "        return np.array(flat, dtype=np.int64)\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        self._reset_internal_state()\n",
    "        observations = {agent: self._board_to_obs() for agent in self.agents}\n",
    "        infos = {agent: {} for agent in self.agents}\n",
    "        return observations, infos\n",
    "\n",
    "    def step(self, actions):\n",
    "        if not self.agents:\n",
    "            return {}, {}, {}, {}, {}\n",
    "\n",
    "        rewards = {agent: 0.0 for agent in self.agents}\n",
    "        terminations = {agent: False for agent in self.agents}\n",
    "        truncations = {agent: False for agent in self.agents}\n",
    "        infos = {agent: {} for agent in self.agents}\n",
    "        winner = None\n",
    "\n",
    "        for agent, token in ((\"player_o\", PLAYER_O), (\"player_x\", PLAYER_X)):\n",
    "            if agent not in actions:\n",
    "                raise KeyError(f\"Missing action for {agent}\")\n",
    "\n",
    "            col = int(actions[agent])\n",
    "            opponent = self._opponent(agent)\n",
    "\n",
    "            if col < 0 or col >= COLUMNS:\n",
    "                rewards[agent] = -1.0\n",
    "                rewards[opponent] = 1.0\n",
    "                terminations = {a: True for a in self.agents}\n",
    "                infos[agent][\"invalid_action\"] = \"out_of_bounds\"\n",
    "                break\n",
    "\n",
    "            if column_is_full(self.grid, col):\n",
    "                rewards[agent] = -1.0\n",
    "                rewards[opponent] = 1.0\n",
    "                terminations = {a: True for a in self.agents}\n",
    "                infos[agent][\"invalid_action\"] = \"column_full\"\n",
    "                break\n",
    "\n",
    "            row = drop_token(self.grid, col, token)\n",
    "            self.last_moves[agent] = (row, col)\n",
    "\n",
    "            if has_winner(self.grid, row, col, token):\n",
    "                winner = agent\n",
    "                rewards[agent] = 1.0\n",
    "                rewards[opponent] = -1.0\n",
    "                terminations = {a: True for a in self.agents}\n",
    "                break\n",
    "        else:\n",
    "            self.timestep += 1\n",
    "            if board_is_full(self.grid) or self.timestep >= self.maxsteps:\n",
    "                truncations = {agent: True for agent in self.agents}\n",
    "\n",
    "        if winner is not None:\n",
    "            for agent in self.agents:\n",
    "                infos[agent][\"winner\"] = agent == winner\n",
    "\n",
    "        observations = {agent: self._board_to_obs() for agent in self.agents}\n",
    "        for agent in self.agents:\n",
    "            if self.last_moves[agent] is not None:\n",
    "                infos[agent][\"last_move\"] = self.last_moves[agent]\n",
    "\n",
    "        return observations, rewards, terminations, truncations, infos\n",
    "\n",
    "    def render(self):\n",
    "        for row in self.grid:\n",
    "            print(\" \".join(str(cell) for cell in row))\n",
    "\n",
    "    def observation_space(self, agent):\n",
    "        return self.observation_spaces[agent]\n",
    "\n",
    "    def action_space(self, agent):\n",
    "        return self.action_spaces[agent]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
